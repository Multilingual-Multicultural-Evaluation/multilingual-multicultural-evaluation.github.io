<html>

<head>
    <title>MME Workshop</title>
    <meta charset="UTF-8">
    <link rel="stylesheet" type="text/css" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap"
        rel="stylesheet">
    <link rel="icon" type="image/svg+xml" href="assets/favicon.svg">
</head>

<body style="padding: 0; margin: 0px;">
    <div id="content"
        style="width: 1100px; margin-left: auto; margin-right: auto; background-color: #eee; padding: 20px; text-align: justify; padding-bottom: 50px;">

        <marquee>This website is under construction. The information here might not be 100% accurate.</marquee>
        <img src="assets/logo.svg"
            style="width: 650px; margin-bottom: 10px; margin-top: 50px; display: block; margin-left: auto; margin-right: auto;">
        <div style="text-align: center;">Workshop co-located with EACL 2026 in Morocco</div>

        <br>
        <br>
        <br>
        <br>


        <h2>Introduction</h2>

        Large language models are undeniably reshaping language technology.
        Yet as models claim to "support X languages," the research community still lacks clear answers to core
        questions, such as
        <i>What does multilinguality really mean, and how should we evaluate it?</i>
        Simple counts of supported languages or translated benchmarks surely aren't enough.
        Languages carry culture, values, and lived experience—dimensions that standard metrics often overlook.
        As a result, multilingual evaluation today suffers from duplicated efforts, inconsistent practices, and limited
        comparability across studies.

        This workshop aims to bring the community together to address these challenges through three goals:
        <ul>
            <li>Community: Create a dedicated venue for multilingual evaluation, data, and methodology.</li>
            <li>Science: Advance and standardize evaluation practices to improve accuracy, scalability, and fairness.
            </li>
            <li>Vision: Integrate cultural and social perspectives into multilingual evaluation.</li>
        </ul>

        LLMs in every language? Prove it. Showcase your work on rigorous, efficient, scalable, culture-aware
        multilingual benchmarking.

        <h2>Call for Papers</h2>

        We welcome archival and non-archival submissions resulting in poster presentations for accepted papers (published in ACL Anthology).
        The topics include but are not limited to:

        <ul>
            <li>Evaluation resources beyond English or Western-centric perspectives and materials</li>
            <li>Annotation methodology and procedures</li>
            <li>Evaluation protocols: ranking vs direct assessment, rubric-based vs reference-based vs reference-free,
                prompt variations, etc</li>
            <li>Complex tasks: multimodality, fairness, long I/O, tool using, code-switching, literary, etc;</li>
            <li>Sociocultural and cognitive variation affecting the use and evaluation across languages;</li>
            <li>Scalable evaluation of cultural and factual knowledge;</li>
            <li>Efficient evaluation of a massive number of languages and tasks;</li>
            <li>Metrics, LLM judges, and reward models;</li>
            <li>Standardised reporting, and scientific comparison of multilingual performance;</li>
            <li>AI-assisted evaluation: data, methods, metrics, and standards</li>
        </ul>

        Submissions should be formatted according to the <a href="https://github.com/acl-org/acl-style-files">ACL style guidelines</a> and should either be up to 4 or 8 pages (short and long papers, respectively), excluding references and appendices.
        Upon acceptance, an additional page will be granted for camera-ready versions.

        Please submit your work through <a href="TODO">this OpenReview link</a> by <span class="highlight">TODO</span>.
        Submissions with an existing ARR meta-review by <span class="highlight">TODO</span> are also elligible.
        For their consideration, please TODO.

        <h2>Program</h2>

        TODO
        
        Please observe the following key dates, AoE:
        <ul>
            <li>Submission deadline: TODO</li>
            <li>Notification: TODO</li>
            <li>Camera-ready: TODO</li>
            <li>Workshop: TODO</li>
        </ul>

        <h2>Organization</h2>

        Alphabetically, the workshop organizers are:
        <ul>
            <li>Alham Fikri Aji (MBZUAI)</li>
            <li>Alexandra Birch (University of Edinburgh)</li>
            <li>Pinzhen Chen (University of Edinburgh)</li>
            <li>Barry Haddow (University of Edinburgh)</li>
            <li>Sara Hooker (Cohere Labs)</li>
            <li>Hanxu Hu (University of Zurich)</li>
            <li>Simran Khanuja (CMU)</li>
            <li>Rico Sennrich (University of Zurich)</li>
            <li>Wenhao Zhu (ByteDance)</li>
            <li>Vilém Zouhar (ETH Zurich)</li>
        </ul>

        Please reach out to <a href="mailto:TODO">TODO</a> with any questions or inquiries.

        This workshop follows ACL's <a
            href="https://www.aclweb.org/adminwiki/index.php?title=Anti-Harassment_Policy">Anti-Harassment Policy</a>.
    </div>
</body>

</html>